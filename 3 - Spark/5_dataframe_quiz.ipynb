{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with DataFrames Coding Quiz\n",
    "\n",
    "Use this Jupyter notebook to find the answers to the quiz in the previous section. There is an answer key in the next part of the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# TODOS: \n",
    "# 1) import any other libraries you might need\n",
    "# 2) instantiate a Spark session \n",
    "# 3) read in the data set located at the path \"data/sparkify_log_small.json\"\n",
    "# 4) write code to answer the quiz questions \n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import sum, count, avg, col, udf, asc, desc, sort_array\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 11:29:50 WARN Utils: Your hostname, Erichs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.15.5 instead (on interface en0)\n",
      "22/04/18 11:29:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/18 11:29:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Exercises 1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"data/sparkify_log_small.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\" (empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|Submit Downgrade|\n",
      "|       Downgrade|\n",
      "|          Logout|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|        NextSong|\n",
      "|         Upgrade|\n",
      "|           Error|\n",
      "|  Submit Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/15 12:56:52 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 260827 ms exceeds timeout 120000 ms\n",
      "22/04/15 12:56:52 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 1\n",
    "\n",
    "# prepare dataset with pages that usedId = '' visited\n",
    "visits = df.filter(df.userId == '') \\\n",
    "    .select(col('page') \\\n",
    "    .alias('visited')) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# get all pages\n",
    "all_pages = df.select('page').dropDuplicates()\n",
    "\n",
    "# prepare join condition\n",
    "cond = (all_pages['page'] == visits['visited'])\n",
    "\n",
    "# left join datasets to find records that exist in\n",
    "# all_pages but not in visits\n",
    "result = all_pages.join(visits, cond, 'left') \\\n",
    "            .select(all_pages.page, visits.visited)\n",
    "\n",
    "result.filter(result.visited.isNull()) \\\n",
    "        .select(col('page')) \\\n",
    "        .show()\n",
    "\n",
    "\n",
    "# result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "What type of user does the empty string user id most likely refer to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should represent users who are not signed up yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: write your code to answer question 3\n",
    "df.filter(df.gender == 'F') \\\n",
    "    .select('userId', 'gender') \\\n",
    "    .distinct() \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  Artist|Artist_qty|\n",
      "+--------+----------+\n",
      "|Coldplay|        83|\n",
      "+--------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 4\n",
    "\n",
    "df.filter(df.page == 'NextSong') \\\n",
    "    .select('Artist') \\\n",
    "    .groupBy('Artist') \\\n",
    "    .agg({'Artist':'count'}) \\\n",
    "    .withColumnRenamed('count(Artist)', 'Artist_qty') \\\n",
    "    .sort(desc('Artist_qty')) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count(period))|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: write your code to answer question 5\n",
    "\n",
    "ishome = udf(lambda homepg : int(homepg == 'Home'), IntegerType())\n",
    "\n",
    "window_open = Window \\\n",
    "    .partitionBy('userID') \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "checker = df.filter((df.page == 'Home') | (df.page == 'NextSong')) \\\n",
    "    .select('userID', 'page', 'ts') \\\n",
    "    .withColumn('fl_home_visit', ishome(col('page'))) \\\n",
    "    .withColumn('period', sum('fl_home_visit').over(window_open))\n",
    "\n",
    "checker.filter((checker.page == 'NextSong')) \\\n",
    "    .groupBy('userID', 'period') \\\n",
    "    .agg({'period':'count'}) \\\n",
    "    .agg({'count(period)':'avg'}) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
